import org.apache.hadoop.fs.Path
import org.apache.hadoop.mapreduce.Job
import org.apache.hadoop.mapreduce.lib.input._

import org.apache.log4j._

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

class HourPartitioner(numParts: Int) extends Partitioner {
  override def numPartitions: Int = numParts

  override def getPartition(key: Any): Int = {
    val path = key.toString
    val hour = path.substring(0,path.lastIndexOf('/'))
    val code = (hash.hashCode % numPartitions)
    if (code < 0) {
      code + numPartitions
    } else {
      code
    }
  }

  override def equals(other: Any): Boolean => other match {
    case hp: HourPartitioner => hp.numPartitions == numPartitions
    case _ => false
  }
}

object ReadWithFilesContext {

  private[this] val logger = Logger.getLogger(getClass.getName);

  def main(args: Array[String]) {
    val conf = new SparkConf().setAppName("Read data with file context")
    val sc = new SparkContext(conf)
    val job = new Job()

    logger.info("Args: " + args.mkString(" "))

    FileInputFormat.addInputPath(job, new Path(args(0)))

    val inF = classOf[TextInputFormat].newInstance
    val initialSplits = sc.broadcast(inF.getSplits(job))

    // Read the data
    val trades = sc.newAPIHadoopFile(args(0), classOf[TextInputFormat], 
      classOf[LongWritable], classOf[Text])
    
    // Add split context
    val tradesFiles = trades.mapPartitionsWithIndex { 
      (idx, iter) => { 
        val split = initialSplits.value
        iter.map { 
          case (l,t) => (split.get(idx).toString, t.toString) 
        }
      }
    }

    // Partition according to hour
    val tradesPartitionedByHour = tradesFiles.partitionBy(HourPartitioner)
    
    println(tradesPartitionedByHour.partitions)

  }
}
